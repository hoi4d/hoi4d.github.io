
<!DOCTYPE html>
<html>
<head>
<title>HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" integrity="sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
<style>
h2 {
  font-family: 'Rubik', sans-serif;
  font-size: 40px;
  font-weight: 300;
  letter-spacing: -1px;
  margin-bottom: 1rem;
}
h3 {
  margin-bottom: 1rem;
}
h4 {
  font-size: 20px;
  margin-bottom: 1rem;
}
video {
  width: 100%;
  height: 100%;
}
code {
  background-color: #f5f5f5;
  display: block;
  padding: 20px;
  white-space: pre-wrap;
}
.container {
  padding: 40px 15px;
}
.center {
  text-align: center;
}
.underline {
  text-decoration: underline;
}
.nowrap {
  white-space: nowrap;
}
.authors {
  line-height: 2;
  font-size: 18px;
}
.section {
  padding: 10px 0 30px;
}
.content {
  padding: 10px 0;
}
.video {
  padding: 20px 0 20px;
}
.image{
  padding: 0 30px;
}
.embedded-video {
  position: relative;
  width: 100%;
  height: 0;
  padding-bottom: 56.25%;
  overflow: hidden;
  border-radius: 10px !important;
}
.embedded-video iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}
.aligncenter {
    clear: both;
    display: block;
    margin: auto;
}
</style>
</head>
<body>
<div class="container">
  <div class="row">
    <div class="col-12">
      <div class="content">
        <h2 class="center content">HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction</h2>
        <div class="center content authors">
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="nowrap">Yunze Liu*<sup>1,3</sup></span>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="nowrap">Yun Liu*<sup>1</sup></span>

          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

            <span class="nowrap">Che Jiang<sup>1</sup></span>
  
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

            <span class="nowrap">Kangbo Lyu<sup>1</sup></span>
 
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
       
            <span class="nowrap">Weikang Wan<sup>2</sup></span>

          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         
            <span class="nowrap">Hao Shen<sup>2</sup></span>
    
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         
            <span class="nowrap">Boqiang Liang<sup>2</sup></span>

          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         
            <span class="nowrap">Zhoujie Fu<sup>1</sup></span>

          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        
            <span class="nowrap">He Wang<sup>2</sup></span>
 
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         
            <span class="nowrap">Li Yiâ€ <sup>1,3</sup></span>


        </div>
        <div class="center content authors">
          <span><sup>1</sup>Tsinghua University, <sup>2</sup>Peking University, <sup>3</sup>Shanghai Qi Zhi Institute</span>
        </div>
        <div class="center authors">
          <span>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022</span>
        </div>
      
      </div>
<iframe width="1120" height="630" src="https://www.youtube.com/embed/yzNqm0JISU0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    
 <!--       <iframe width="99%" height="618" src="https://www.youtube.com/embed/S8Amc6D8SKY?autoplay=1" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
<!--       <div class="video"> -->
<!--         <video autoplay="autoplay" muted="muted" loop="loop"> -->
<!--           <source src="video.mp4" type="video/mp4"> -->
<!--         </video> -->
      </div>

      <div class="section">
        <h3>Abstract</h3>
        <p>We present HOI4D, a large-scale 4D egocentric dataset with rich annotations, to catalyze the research of category-level human-object interaction. HOI4D consists of 2.4M RGB-D egocentric video frames over 4000 sequences collected by 9 participants interacting with 800 different object instances from 16 categories over 610 different indoor rooms. Frame-wise annotations for panoptic segmentation, motion segmentation, 3D hand pose, category-level object pose and hand action have also been provided, together with reconstructed object meshes and scene point clouds. With HOI4D, we establish three benchmarking tasks to promote category-level HOI from 4D visual signals including semantic segmentation of 4D dynamic point cloud sequences, category-level object pose tracking, and egocentric action segmentation with diverse interaction targets. In-depth analysis shows HOI4D poses great challenges to existing methods and produces great research opportunities.</p>
      </div>

      <div class="section">
        <h3>Overview of HOI4D</h3>
        <p>We construct a large-scale 4D egocentric dataset with rich annotation for category-level human-object interaction. Frame-wise annotations for action segmentation(a), motion segmentation(b), panoptic segmentation(d), 3D hand pose and category-level object pose(c) are provided, together with reconstructed object meshes(e) and scene point cloud. </p>


        <img src="teaser.png" width="700" height="432" class="aligncenter">
          
      </div>

      <div class="section">
        <h3>Object categories</h3>
         <img src="class.png" width="800" height="432" class="aligncenter">
      </div>
    
      <div class="section">
        <h3>Tasks and Benchmarks</h3>
         <h4> Category-Level Object and Part Pose Tracking</h4>
         <img src="pose.png" width="800" height="432" class="aligncenter">
         <h4> 4D Point Cloud Videos Semantic Segmentation</h4>
         <img src="4Dseg.png" width="800" height="432" class="aligncenter">
         <h4> Fine-grained Video Action Segmentation</h4>
         <img src="actionseg.png" width="800" height="432" class="aligncenter">
      </div>



      <div class="section">
        <h3>Paper</h3>
        <div class="row content">
          <div class="center image">
            <a href="HOI4D_cvpr2022.pdf" target="_blank">
              <br>
              <span class="icon"> <i class="fas fa-file-pdf"></i> </span>
              <span>PDF</span>
            </a>
          </div>
          <div class="center image">
            <a href="supp_cvpr2022.pdf" target="_blank">
              <br>
              <span class="icon"> <i class="fas fa-file-pdf"></i> </span>
              <span>Supplementary</span>
            </a>
          </div>
          <div class="center image">
            <a href="https://arxiv.org/abs/2203.01577" target="_blank">
              <br>
              <span class="icon"> <i class="fas fa-file-pdf"></i> </span>
              <span>arXiv</span>
            </a>
          </div>
        </div>
        <br>
        <div class="content">
          <h4>Citing HOI4D</h4>
          <p>Please cite HOI4D if it helps your research:</p>
          <pre><code>@inproceedings{liu2022hoi4d,
  title={HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction},
  author={Liu, Yunze and Liu, Yun and Jiang, Che and Lyu, Kangbo and Wan, Weikang and Shen, Hao and Liang, Boqiang and Fu, Zhoujie and Wang, He and Yi, Li},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={21013--21022},
  year={2022}
}</code></pre>
        </div>
      </div>

      <div class="section">
        <h3>Data</h3>
        <p>HOI4D_color.<a href="https://drive.google.com/file/d/1pL9V_x5tWk1QFjT7Dhiij1W-AXARyhPP/view?usp=sharing" target="_blank"><span class="icon"> <i class="fas fa-download"></i> </span>RGB_video</a>. </p>
        <p>HOI4D_depth.<a href="https://drive.google.com/file/d/1tIoxkd2YBkGxORbVajp6JMlaicCL5QfK/view?usp=sharing" target="_blank"><span class="icon"> <i class="fas fa-download"></i> </span>Depth_video</a>. </p>
        <p>HOI4D_CAD_models.<a href="https://drive.google.com/file/d/1evpTI51UpDg_a1kAGXQdkfSd0DlI1_Gl/view?usp=sharing" target="_blank"><span class="icon"> <i class="fas fa-download"></i> </span>CAD_model</a>. </p>
        <p>HOI4D_annotations.<a href="https://drive.google.com/file/d/1rQVoYsAilwmq66ilGj6d2Q2fUF7osrpn/view?usp=sharing" target="_blank"><span class="icon"> <i class="fas fa-download"></i> </span>Annotations</a>. </p>
        <p>HOI4D_cameras.<a href="https://drive.google.com/file/d/1mQaPw_dVLYE31BM3_2M_gHoo_ufAJ6kw/view?usp=sharing" target="_blank"><span class="icon"> <i class="fas fa-download"></i> </span>Camera_parameters</a>. </p>
        <p>Instructions and hand pose is coming very soon.<p>
      </div>


      <div class="section">
        <h3>Contact</h3>
        <p>Send any comments or questions to Yunze Liu or Yun Liu: liuyzchina@gmail.com, yun-liu22@mails.tsinghua.edu.cn. HOI4D is licensed under <span class="underline"><a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span>.</p>
      </div>

      <hr>
      <p>Last updated on 2022/08/10</p>
    </div>
  </div>
</div>
</body>
</html>
